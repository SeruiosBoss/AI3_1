{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeruiosBoss/AI3_1/blob/main/11/Практическая_работа_11_Ильнур.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3DZeo7xi2HL"
      },
      "source": [
        "# Практическая работа 11. Метрические методы классификации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ФИО: **Мухамеджанов Ильнур Тимурович**\n",
        "\n",
        "Группа: **ПИН-б-о-22-1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIyzax9q7JF"
      },
      "source": [
        "## Задание, Вариант 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1UMdzQGjFix"
      },
      "source": [
        "1. Выполните `построение модели` классификации на основе `метода\n",
        "ближайших соседей`. В ходе решения задачи необходимо решить следующие\n",
        "подзадачи:\n",
        "2. Построение `классификатора с заданием K` (количества ближайших\n",
        "соседей) пользователем;\n",
        "3. `Вычисление оценки hold-out` для различнх значений K, а также для\n",
        "различных долей обучающей и тестирующей подвыборок;\n",
        "4. `Вычисление оценки cross validation` для различных значений K, а также\n",
        "для различных значений `fold` (количества подмножеств при кросс-валидации).\n",
        "5. Вычислите `оптимальные значения K`. Обоснуйте свой выбор.\n",
        "`Продемонстрируйте` использование полученного классификатора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGlwZaXQrAfd"
      },
      "source": [
        "## Обозначение функций, Построение модели `методом ближайших соседей`, задание `K`, вычисление оценок `hold-out` и `cross validation` с различными значениями `fold`, `оптимальные` значения K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzShFx3l0l49",
        "outputId": "8a4fddd3-38f1-41cf-f1d0-0be0bccc594f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Загрузка данных\n",
        "attributes = [\n",
        "    \"class\", \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises\", \"odor\", \"gill-attachment\",\n",
        "    \"gill-spacing\", \"gill-size\", \"gill-color\", \"stalk-shape\", \"stalk-root\",\n",
        "    \"stalk-surface-above-ring\", \"stalk-surface-below-ring\", \"stalk-color-above-ring\",\n",
        "    \"stalk-color-below-ring\", \"veil-type\", \"veil-color\", \"ring-number\", \"ring-type\",\n",
        "    \"spore-print-color\", \"population\", \"habitat\"\n",
        "]\n",
        "\n",
        "data = pd.read_csv(\"agaricus-lepiota.data\", header=None, names=attributes)\n",
        "\n",
        "# Преобразование данных\n",
        "# Преобразуем категориальные данные в числовые с помощью one-hot encoding\n",
        "data_encoded = pd.get_dummies(data.drop('class', axis=1))\n",
        "labels = data['class'].map({'e': 0, 'p': 1})  # Съедобные = 0, Ядовитые = 1\n",
        "\n",
        "# 1. Построение классификатора\n",
        "k_values = range(1, 21)\n",
        "holdout_accuracies = []\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки (hold-out method)\n",
        "train_sizes = [0.6, 0.7, 0.8]\n",
        "optimal_k = {}\n",
        "\n",
        "for train_size in train_sizes:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data_encoded, labels, test_size=1-train_size, random_state=42\n",
        "    )\n",
        "\n",
        "    accuracies = []\n",
        "    for k in k_values:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        knn.fit(X_train, y_train)\n",
        "        y_pred = knn.predict(X_test)\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    holdout_accuracies.append(accuracies)\n",
        "    optimal_k[train_size] = k_values[np.argmax(accuracies)]\n",
        "\n",
        "# 2. Построение графика точности hold-out\n",
        "plt.figure(figsize=(10, 6))\n",
        "for idx, train_size in enumerate(train_sizes):\n",
        "    plt.plot(k_values, holdout_accuracies[idx], label=f'Train size {train_size}')\n",
        "\n",
        "plt.title('Hold-out accuracy for different K values')\n",
        "plt.xlabel('K (Number of neighbors)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 3. Кросс-валидация\n",
        "folds = [3, 5, 10]\n",
        "cross_val_accuracies = {}\n",
        "\n",
        "for fold in folds:\n",
        "    fold_accuracies = []\n",
        "    for k in k_values:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        scores = cross_val_score(knn, data_encoded, labels, cv=fold, scoring='accuracy')\n",
        "        fold_accuracies.append(scores.mean())\n",
        "\n",
        "    cross_val_accuracies[fold] = fold_accuracies\n",
        "\n",
        "# Построение графика точности кросс-валидации\n",
        "plt.figure(figsize=(10, 6))\n",
        "for fold, accuracies in cross_val_accuracies.items():\n",
        "    plt.plot(k_values, accuracies, label=f'{fold}-fold CV')\n",
        "\n",
        "plt.title('Cross-validation accuracy for different K values')\n",
        "plt.xlabel('K (Number of neighbors)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 4. Оптимальное значение K\n",
        "cv_optimal_k = {fold: k_values[np.argmax(acc)] for fold, acc in cross_val_accuracies.items()}\n",
        "print(\"Оптимальные значения K для hold-out:\", optimal_k)\n",
        "print(\"Оптимальные значения K для cross-validation:\", cv_optimal_k)\n",
        "\n",
        "# 5. Демонстрация использования оптимального классификатора\n",
        "final_k = cv_optimal_k[5]  # Например, выбираем оптимальное K для 5-fold CV\n",
        "final_knn = KNeighborsClassifier(n_neighbors=final_k)\n",
        "final_knn.fit(X_train, y_train)\n",
        "final_accuracy = accuracy_score(y_test, final_knn.predict(X_test))\n",
        "\n",
        "print(f\"Точность финального классификатора с K={final_k}: {final_accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfm23877A4o2"
      },
      "source": [
        "В данной работе были использованы библиотеки `numpy, matplotlib, pandas, sklearn` и модули `pyplot, train_test_split, cross_val_score, KNeighborsClassifier accuracy_score` для построения модели методом ближайших соседей, задания `K`, вычисления оценок `hold-out` и `cross validation` с различными значениями `fold`, `оптимальные` значения K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgydWXtBqRUl"
      },
      "source": [
        "### 1. **Поясните особенности основных методов метрической классификации: метод ближайшего соседа, метод \\(k\\) ближайших соседей.**  \n",
        "- **Метод ближайшего соседа:**  \n",
        "  - Для классификации нового объекта определяется ближайший объект в обучающей выборке на основе выбранной метрики расстояния (например, евклидово расстояние).  \n",
        "  - Класс нового объекта совпадает с классом ближайшего соседа.  \n",
        "  - Простой, но чувствительный к шуму метод.  \n",
        "\n",
        "- **Метод \\(k\\) ближайших соседей (kNN):**  \n",
        "  - Для классификации нового объекта рассматриваются \\(k\\) ближайших соседей.  \n",
        "  - Класс определяется голосованием: выбирается класс, который чаще встречается среди соседей.  \n",
        "  - Уменьшает влияние шума в данных по сравнению с методом ближайшего соседа.  \n",
        "  - Требует выбора параметра \\(k\\) и метрики расстояния.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Поясните основные принципы и этапы реализации метода kNN.**  \n",
        "1. **Выбор параметров:**  \n",
        "   - Определить \\(k\\) (количество соседей).  \n",
        "   - Выбрать метрику расстояния (евклидово, манхэттенское и др.).  \n",
        "\n",
        "2. **Сбор и подготовка данных:**  \n",
        "   - Разделить данные на обучающую и тестовую выборки.  \n",
        "   - Нормализовать или стандартизировать данные (особенно при использовании метрик, чувствительных к масштабу).  \n",
        "\n",
        "3. **Для каждого объекта тестовой выборки:**  \n",
        "   - Вычислить расстояния до всех объектов обучающей выборки.  \n",
        "   - Найти \\(k\\) ближайших соседей.  \n",
        "   - Определить класс нового объекта на основе голосования соседей.  \n",
        "\n",
        "4. **Оценка качества модели:**  \n",
        "   - Использовать метрики точности, F1-меру или ROC-AUC.  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Поясните принцип выбора количества соседних объектов \\(k\\), по которым определяется принадлежность целевого объекта к результирующему классу.**  \n",
        "- **Малое \\(k\\):**  \n",
        "  - Модель становится чувствительной к шуму.  \n",
        "  - Риск переобучения (high variance).  \n",
        "\n",
        "- **Большое \\(k\\):**  \n",
        "  - Снижается влияние шума.  \n",
        "  - Модель может быть слишком обобщенной (high bias).  \n",
        "\n",
        "- **Оптимальный выбор \\(k\\):**  \n",
        "  - Зависит от объема и структуры данных.  \n",
        "  - Обычно \\(k\\) выбирают на основе кросс-валидации.  \n",
        "  - Часто используется \\(k = \\sqrt{N}\\), где \\(N\\) — размер обучающей выборки.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. **В чем заключается метод парзеновского окна?**  \n",
        "- **Суть метода:**  \n",
        "  - Это метод оценки плотности распределения.  \n",
        "  - Вместо фиксированного числа соседей (\\(k\\)) используется фиксированное окно (радиус).  \n",
        "  - Класс определяется на основе количества объектов в пределах окна.  \n",
        "\n",
        "- **Формула плотности:**  \n",
        "  \\[\n",
        "  f(x) = \\frac{1}{Nh} \\sum_{i=1}^{N} K\\left(\\frac{x - x_i}{h}\\right)\n",
        "  \\]  \n",
        "  где \\(h\\) — ширина окна, \\(K\\) — ядро (например, гауссово).  \n",
        "\n",
        "- **Особенность:**  \n",
        "  - Ширина окна сильно влияет на результат:  \n",
        "    - Узкое окно учитывает только ближайших соседей.  \n",
        "    - Широкое окно захватывает больше объектов, но снижает точность локального предсказания.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Поясните принцип метода потенциальных функций.**  \n",
        "- **Суть метода:**  \n",
        "  - Каждому объекту в обучающей выборке присваивается **потенциальная функция**.  \n",
        "  - Потенциал зависит от расстояния между объектами:  \n",
        "    - Близкие к целевому объекту точки имеют больший вес.  \n",
        "  - Класс нового объекта определяется суммой вкладов (потенциалов) соседей.  \n",
        "\n",
        "- **Формула:**  \n",
        "  \\[\n",
        "  f(x) = \\sum_{i=1}^{N} \\alpha_i K(||x - x_i||)\n",
        "  \\]  \n",
        "  где \\(\\alpha_i\\) — потенциалы, \\(K\\) — ядро, \\(||x - x_i||\\) — расстояние.  \n",
        "\n",
        "- **Особенность:**  \n",
        "  - Метод хорошо работает в условиях шумных данных.  \n",
        "  - Сложность — в выборе подходящего ядра и параметров потенциала.  \n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Какие параметры оптимизируют в методах kNN?**  \n",
        "1. **Количество соседей (\\(k\\)):**  \n",
        "   - Определяет баланс между переобучением и недообучением.  \n",
        "\n",
        "2. **Метрика расстояния:**  \n",
        "   - Евклидово расстояние, манхэттенское, косинусное и др.  \n",
        "   - Выбор метрики зависит от природы данных (например, для категориальных признаков подходят метрики на основе схожести).  \n",
        "\n",
        "3. **Вес соседей:**  \n",
        "   - Равномерные веса или взвешивание в зависимости от расстояния.  \n",
        "\n",
        "4. **Метод предобработки данных:**  \n",
        "   - Нормализация или стандартизация (особенно для метрик, чувствительных к масштабу)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP8QFeWhqyUPlS4P3jU6FYS",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
