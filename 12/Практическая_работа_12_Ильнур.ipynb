{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enikolaev/AI_in_biotech/blob/main/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3DZeo7xi2HL"
      },
      "source": [
        "# Практическая работа 12. Логические методы классификации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ФИО: **Мухамеджанов Ильнур Тимурович**\n",
        "\n",
        "Группа: **ПИН-б-о-22-1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIyzax9q7JF"
      },
      "source": [
        "## Задание, Вариант 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1UMdzQGjFix"
      },
      "source": [
        "1. Выполните построение модели классификации на основе `дерева\n",
        "классификации`. В ходе решения задачи необходимо решить следующие подзадачи:\n",
        "2. Построение `логического классификатора` с заданием `max_depth`\n",
        "(максимальной глубины) и ``max_features` (максимального количества признаков)\n",
        "пользователем (установить любые); `визуализация дерева` решений для выбранных\n",
        "исследователем параметров (в формате .png)\n",
        "3. Вычисление оценки `cross validation (MSE)` для различнх значений\n",
        "max_depth (построить график зависимости);\n",
        "4. Вычисление оценки `cross validation (MSE)` для различнх значений\n",
        "max_features (построить график зависимости);\n",
        "5. Вычислите оптимальные значения `max_depth` и `max_features`. Обоснуйте\n",
        "свой выбор. Продемонстрируйте использование полученного классификатора.\n",
        "6. Выведите `дерево` в формате `.png`;\n",
        "7. Выведите `решающие границы` полученной модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGlwZaXQrAfd"
      },
      "source": [
        "## Обозначение функций, Построение модели классификации на основе `дерева классификации`, `логического классификатора`, оценок `max_depth, max_features`, вывод дерева в формате `.png`, вывод `решающих границ` полученной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzShFx3l0l49",
        "outputId": "8a4fddd3-38f1-41cf-f1d0-0be0bccc594f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "# Загрузка данных\n",
        "attributes = [\n",
        "    \"class\", \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises\", \"odor\", \"gill-attachment\",\n",
        "    \"gill-spacing\", \"gill-size\", \"gill-color\", \"stalk-shape\", \"stalk-root\",\n",
        "    \"stalk-surface-above-ring\", \"stalk-surface-below-ring\", \"stalk-color-above-ring\",\n",
        "    \"stalk-color-below-ring\", \"veil-type\", \"veil-color\", \"ring-number\", \"ring-type\",\n",
        "    \"spore-print-color\", \"population\", \"habitat\"\n",
        "]\n",
        "\n",
        "data = pd.read_csv(\"agaricus-lepiota.data\", header=None, names=attributes)\n",
        "\n",
        "# Преобразование данных\n",
        "# Преобразуем категориальные данные в числовые с помощью one-hot encoding\n",
        "data_encoded = pd.get_dummies(data.drop('class', axis=1))\n",
        "labels = data['class'].map({'e': 0, 'p': 1})  # Съедобные = 0, Ядовитые = 1\n",
        "\n",
        "# 1. Построение классификатора\n",
        "max_depth_values = range(1, 21)\n",
        "max_features_values = range(1, len(data_encoded.columns) + 1, 5)\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data_encoded, labels, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Построение и визуализация дерева решений\n",
        "chosen_max_depth = 5\n",
        "chosen_max_features = 10\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(max_depth=chosen_max_depth, max_features=chosen_max_features, random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Визуализация дерева решений\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_classifier, feature_names=data_encoded.columns, class_names=['Edible', 'Poisonous'], filled=True)\n",
        "plt.title(\"Дерево решений\")\n",
        "plt.savefig(\"decision_tree.png\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Cross-validation для max_depth\n",
        "cv_mse_depth = []\n",
        "for depth in max_depth_values:\n",
        "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    mse = -cross_val_score(dt, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
        "    cv_mse_depth.append(mse)\n",
        "\n",
        "# График зависимости MSE от max_depth\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depth_values, cv_mse_depth, marker='o', label='Cross-validation MSE')\n",
        "plt.title('Зависимость MSE от max_depth')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 3. Cross-validation для max_features\n",
        "cv_mse_features = []\n",
        "for features in max_features_values:\n",
        "    dt = DecisionTreeClassifier(max_features=features, random_state=42)\n",
        "    mse = -cross_val_score(dt, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
        "    cv_mse_features.append(mse)\n",
        "\n",
        "# График зависимости MSE от max_features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_features_values, cv_mse_features, marker='o', label='Cross-validation MSE')\n",
        "plt.title('Зависимость MSE от max_features')\n",
        "plt.xlabel('max_features')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 4. Оптимальные значения max_depth и max_features\n",
        "optimal_max_depth = max_depth_values[np.argmin(cv_mse_depth)]\n",
        "optimal_max_features = max_features_values[np.argmin(cv_mse_features)]\n",
        "\n",
        "print(f\"Оптимальное значение max_depth: {optimal_max_depth}\")\n",
        "print(f\"Оптимальное значение max_features: {optimal_max_features}\")\n",
        "\n",
        "# 5. Использование оптимального классификатора\n",
        "final_dt_classifier = DecisionTreeClassifier(max_depth=optimal_max_depth, max_features=optimal_max_features, random_state=42)\n",
        "final_dt_classifier.fit(X_train, y_train)\n",
        "y_pred = final_dt_classifier.predict(X_test)\n",
        "final_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"MSE финального классификатора: {final_mse:.2f}\")\n",
        "\n",
        "# 6. Визуализация решающих границ (только для двух признаков)\n",
        "plt.figure(figsize=(10, 6))\n",
        "display = DecisionBoundaryDisplay.from_estimator(\n",
        "    final_dt_classifier, X_test.iloc[:, :2], response_method=\"predict\", alpha=0.5, cmap=\"coolwarm\"\n",
        ")\n",
        "plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_test, edgecolor=\"k\", cmap=\"coolwarm\")\n",
        "plt.title(\"Решающие границы дерева решений\")\n",
        "plt.xlabel(data_encoded.columns[0])\n",
        "plt.ylabel(data_encoded.columns[1])\n",
        "plt.savefig(\"decision_boundaries.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfm23877A4o2"
      },
      "source": [
        "В данной работе были использованы библиотеки `numpy, matplotlib, pandas, sklearn` и модули `pyplot, DecisionTreeClassifier, plot_tree, cross_val_score, train_test_split, mean_squared_error, DecisionBoundaryDisplay` для построения классификации на основе `дерева классификации`, `логического классификатора`, оценок `max_depth, max_features`, вывод дерева в формате `.png`, вывод `решающих границ` полученной модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgydWXtBqRUl"
      },
      "source": [
        "### 1. **Поясните принцип построения дерева решений.**  \n",
        "- **Суть метода:**  \n",
        "  Дерево решений — это алгоритм, который рекурсивно делит пространство признаков на подмножества, стремясь максимизировать \"чистоту\" (или однородность) данных в каждом подмножестве.  \n",
        "\n",
        "- **Этапы построения:**  \n",
        "  1. **Выбор признака для разбиения:**  \n",
        "     - Используются критерии информативности, такие как энтропия, прирост информации или критерий Джини.  \n",
        "  2. **Разделение данных:**  \n",
        "     - На каждой вершине (узле) данные делятся на подгруппы на основе выбранного признака и порога.  \n",
        "  3. **Рекурсивное построение:**  \n",
        "     - Процесс повторяется для каждой подгруппы до достижения критерия остановки (например, минимального размера узла).  \n",
        "  4. **Присвоение метки класса:**  \n",
        "     - В листьях (конечных узлах) определяется класс на основе большинства объектов.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Укажите статистическое определение информативности.**  \n",
        "- **Информативность признака** — это мера уменьшения неопределенности (энтропии) или улучшения предсказательной способности при использовании признака для разделения данных.  \n",
        "\n",
        "- **Формула прироста информации:**  \n",
        "\\[\n",
        "IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
        "\\]  \n",
        "где \\(H(S)\\) — энтропия исходного набора, \\(H(S_v)\\) — энтропия подмножества, \\(A\\) — признак, \\(S_v\\) — подмножество, полученное разбиением по \\(A\\).  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Поясните энтропийное определение информативности.**  \n",
        "- **Энтропия:**  \n",
        "  Это мера неопределенности или хаотичности данных:  \n",
        "  \\[\n",
        "  H(S) = - \\sum_{i=1}^C p_i \\log_2 p_i\n",
        "  \\]  \n",
        "  где \\(p_i\\) — вероятность класса \\(i\\), \\(C\\) — количество классов.  \n",
        "\n",
        "- **Прирост информации (Information Gain):**  \n",
        "  Это уменьшение энтропии при разбиении данных по признаку:  \n",
        "  \\[\n",
        "  IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
        "  \\]  \n",
        "\n",
        "- **Информативность признака:**  \n",
        "  Чем больше прирост информации, тем более информативным является признак для разделения данных.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Что такое многоклассовая информативность? Для чего она применяется?**  \n",
        "- **Многоклассовая информативность** — это мера информативности признака в задачах классификации с несколькими классами.  \n",
        "\n",
        "- **Применение:**  \n",
        "  Используется для оценки полезности признаков в задачах, где целевая переменная имеет более двух категорий.  \n",
        "\n",
        "- **Пример:**  \n",
        "  Для многоклассовых данных прирост информации или критерий Джини учитывают вероятность принадлежности объекта ко всем классам, а не только к двум.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Поясните назначение и алгоритм бинаризации количественных признаков.**  \n",
        "- **Назначение:**  \n",
        "  Бинаризация количественных признаков превращает их в бинарные значения, что облегчает использование методов, требующих категориальных данных, таких как некоторые варианты деревьев решений или логистической регрессии.  \n",
        "\n",
        "- **Алгоритм:**  \n",
        "  1. **Выбор порога (threshold):**  \n",
        "     - Порог можно выбрать на основе медианы, среднего значения или анализа распределения данных.  \n",
        "  2. **Создание бинарного признака:**  \n",
        "     - Признак принимает значение 1, если его значение больше или равно порогу, и 0 — иначе.  \n",
        "\n",
        "- **Пример:**  \n",
        "  Исходный признак: [3.5, 7.2, 2.1, 5.8], порог = 5.  \n",
        "  После бинаризации: [0, 1, 0, 1].  \n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Поясните порядок поиска закономерностей в форме конъюнкций.**  \n",
        "- **Закономерности в форме конъюнкций:**  \n",
        "  Это правила вида:  \n",
        "  \\[\n",
        "  \\text{Если (условие 1 И условие 2 И ... И условие n), то класс = X.}\n",
        "  \\]  \n",
        "\n",
        "- **Порядок поиска:**  \n",
        "  1. **Генерация кандидатов:**  \n",
        "     - Формируются комбинации условий на основе признаков данных.  \n",
        "  2. **Оценка качества:**  \n",
        "     - Проверяется точность каждого правила на обучающей выборке.  \n",
        "     - Используются метрики, такие как поддержка (support) и достоверность (confidence).  \n",
        "  3. **Фильтрация:**  \n",
        "     - Убираются конъюнкции с низкой поддержкой или достоверностью.  \n",
        "  4. **Оптимизация:**  \n",
        "     - Убираются избыточные условия, не влияющие на результат.  \n",
        "\n",
        "- **Пример:**  \n",
        "  Для данных с признаками \"Возраст\" и \"Доход\" возможная закономерность:  \n",
        "  \\[\n",
        "  \\text{Если (Возраст > 30 И Доход > 50000), то класс = \"Покупатель\".}\n",
        "  \\]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP8QFeWhqyUPlS4P3jU6FYS",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
